{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook is designed to generate explanations for a given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "\n",
    "from utils import parse_response\n",
    "\n",
    "# Load OPENAI_API_KEY from .env file\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the file to apply the explanations for\n",
    "file = \"../data/wdc/train_large/preprocessed_wdcproducts80cc20rnd000un_train_large_domain_simple_free.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_structured_explanations(product_1, product_2, label, custom_id):\n",
    "    \n",
    "    return {\n",
    "        \"custom_id\": custom_id,\n",
    "        \"method\": \"POST\",\n",
    "        \"url\": \"/v1/chat/completions\",\n",
    "        \"body\": {\n",
    "            \"model\": \"gpt-4o\",\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": f\"\"\"\n",
    "                Do the two entity descriptions refer to the same real-world entity?\n",
    "                Entity 1: {product_1}\n",
    "                Entity 2: {product_2}\n",
    "\n",
    "                The correct answer is {label}.\n",
    "\n",
    "                Please provide an explanation for this answer in a structured format, listing the attributes that you compared for reaching this answer. Each attribute should be accompanied by the attribute values and a score between -1 and 1 that shows the importance of the attribute for the decision. If the attribute influenced the decision towards non-match the importance score should be negative. If the attribute pointed towards a match, the importance score should be positive. Also provide a similarity score for the attribute values. If an attribute only occurs in one item, specify the value of that attribute for the other item as \"missing\". An example output is the following:\n",
    "\n",
    "                attribute=brand|||importance=0.05|||values=Logitech###Logitech|||similarity=1.00\n",
    "                attribute=model|||importance=-0.95|||values=MX G500###MX Master 3S|||similarity=0.20\n",
    "                attribute=color|||importance=0.00|||values=missing###Graphite|||similarity=0.00\n",
    "                \n",
    "                Here is a complete example:\n",
    "                Do the two product descriptions refer to the same real-world product? Entity 1: 'WD 4TB Black My Passport Portable External Hard Drive - USB 3.0 - WDBYFT0040BBK-WESN'. Entity 2: 'Dysk WD My Passport 1TB USB 3.0 black'.\n",
    "                \"No. \n",
    "                attribute=brand|||importance=0.05|||values=Western Digital###Western Digital|||similarity=1.00\n",
    "                attribute=model|||importance=0.95|||values=My Passport###My Passport|||similarity=1.00\n",
    "                attribute=storage capacity|||importance=0.9|||values=4TB###1TB|||similarity=0.25\n",
    "                attribute=color|||importance=0.1|||values=Black###Black|||similarity=1.00\n",
    "                attribute=USB version|||importance=0.05|||values=USB 3.0###USB 3.0|||similarity=1.00\n",
    "                \n",
    "                Do not provide a explanation in a different format. The explanation should be in the format described above. Only provide the answer and explanation dont repeat the question.\n",
    "                \"\"\"}\n",
    "            ],\n",
    "            \"max_tokens\": 1000,\n",
    "            \"temperature\": 0\n",
    "        }\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_wadhwa_explanations(product_1, product_2, label, custom_id):\n",
    "    label = \"MATCH\" if label == 1 else \"NOT A MATCH\"\n",
    "    return {\n",
    "        \"custom_id\": custom_id,\n",
    "        \"method\": \"POST\",\n",
    "        \"url\": \"/v1/chat/completions\",\n",
    "        \"body\": {\n",
    "            \"model\": \"gpt-4o-mini\",\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": f\"\"\"\n",
    "                <s>[INST] Given the following two examples, provide an explanation for the third example for why the two entities do or do not match. [\\INST]\n",
    "\n",
    "                Entity A: [NAME] samsung dlp tv stand in black tr72bx [DESCRIPTION] samsung dlp tv stand in black tr72bx designed to fit samsung hlt7288, hlt7288, hl72a650, and hl67a650 television sets tempered 6mm tinted glass shelves wide audio storage shelves to accommodate 4 or more components wire management system easy to assemble high gloss black finish [PRICE] 369.0\n",
    "                Entity B: [NAME] samsung tr72b tv stand [DESCRIPTION] glass black [PRICE] 232.14\n",
    "                Label: MATCH\n",
    "                Explanation: Both entities refer to samsung TV stand in black and therefore have substantially similar specifications, therefore theyâ€™re a match. </s>\n",
    "\n",
    "                Entity A: [NAME] canon high capacity color ink cartridge color ink cl51 [DESCRIPTION] canon high capacity color ink cartridge cl51 compatible with pixma ip6210d, ip6220d, mp150, mp170 and mp450 printers [PRICE] 35.0\n",
    "                Entity B: [NAME] canon pg-40 twin pack black ink cartridge 0615b013 [DESCRIPTION] black [PRICE]\n",
    "                Label: NOT A MATCH\n",
    "                Explanation: Entity A refers to color ink cartridge while Entity B is a black ink cartridge, therefore they are not a match. </s>\n",
    "\n",
    "                Entity A: [NAME] {product_1.get(\"name\")} [DESCRIPTION] {product_1.get(\"description\")} [PRICE] {product_1.get(\"price\")}\n",
    "                Entity B: [NAME] {product_2.get(\"name\")} [DESCRIPTION] {product_2.get(\"description\")} [PRICE] {product_2.get(\"price\")}\n",
    "                Label: {label}\n",
    "                Explanation:\n",
    "                \"\"\"}\n",
    "            ],\n",
    "            \"max_tokens\": 128,\n",
    "            \"temperature\": 0,\n",
    "            \"top_p\": 0.95,\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract the entity strings\n",
    "def extract_entities(text):\n",
    "    entity_1 = text.split(\"Entity 1: '\")[1].split(\"'\")[0]\n",
    "    entity_2 = text.split(\"Entity 2: '\")[1].split(\"'\")[0]\n",
    "    return entity_1, entity_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d7dfe24792c4ca08b56370e0e55bd84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19835 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# set the file to generate explanations for \n",
    "small_df = pd.read_csv(file)\n",
    "\n",
    "# Create the JSONL file with all requests\n",
    "requests = []\n",
    "for index, row in tqdm(small_df.iterrows(), total=small_df.shape[0]):\n",
    "    product_1, product_2 = extract_entities(row[\"prompt\"])\n",
    "    label = row[\"completion\"]\n",
    "    custom_id = str(index)\n",
    "    prompt = generate_structured_explanations(product_1, product_2, label, custom_id=custom_id)\n",
    "    requests.append(prompt)\n",
    "\n",
    "batch_file_path = \"explanation.jsonl\"\n",
    "with open(batch_file_path, \"w\") as f:\n",
    "    for request in requests:\n",
    "        f.write(json.dumps(request) + \"\\n\")\n",
    "        \n",
    "        \n",
    "batch_input_file = client.files.create(\n",
    "    file=open(batch_file_path, \"rb\"),\n",
    "    purpose=\"batch\"\n",
    ")\n",
    "\n",
    "batch_input_file_id = batch_input_file.id\n",
    "\n",
    "batch = client.batches.create(\n",
    "    input_file_id=batch_input_file_id,\n",
    "    endpoint=\"/v1/chat/completions\",\n",
    "    completion_window=\"24h\",\n",
    "    metadata={\"description\": \"Generate structured explanations for the WDC dataset\"}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m client\u001b[38;5;241m.\u001b[39mbatches\u001b[38;5;241m.\u001b[39mretrieve(\u001b[43mbatch\u001b[49m\u001b[38;5;241m.\u001b[39mid)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# download the results\u001b[39;00m\n\u001b[1;32m      4\u001b[0m batch_output_file \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mfiles\u001b[38;5;241m.\u001b[39mretrieve(batch\u001b[38;5;241m.\u001b[39moutput_file_id)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'batch' is not defined"
     ]
    }
   ],
   "source": [
    "client.batches.retrieve(batch.id)\n",
    "\n",
    "# download the results\n",
    "batch_output_file = client.files.retrieve(batch.output_file_id)\n",
    "batch_output_file.download_to_file(\"explanation_output.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_explanations = pd.read_json(\"explanation_output.jsonl\", lines=True)\n",
    "\n",
    "generated_explanations_parsed = generated_explanations[\"response\"].apply(parse_response)    \n",
    "generated_explanations = pd.concat([generated_explanations, generated_explanations_parsed], axis=1)\n",
    "\n",
    "# convert the custom_id to an int\n",
    "generated_explanations[\"custom_id\"] = generated_explanations[\"custom_id\"].astype(int)\n",
    "\n",
    "dataset_without_explanations = pd.read_csv(file)\n",
    "\n",
    "for index, row in dataset_without_explanations.iterrows():\n",
    "    custom_id = index\n",
    "    explanation = generated_explanations[generated_explanations[\"custom_id\"] == custom_id][\"content\"].values[0]\n",
    "    dataset_without_explanations.at[index, \"completion\"] = explanation\n",
    "    \n",
    "dataset_without_explanations.to_csv(file.replace(\".csv\", \"_with_explanation.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
