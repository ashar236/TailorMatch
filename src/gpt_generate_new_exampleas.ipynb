{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate new training examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import numpay as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from utils import parse_response\n",
    "\n",
    "# Load OPENAI_API_KEY from .env file\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the file path to generate new examples \n",
    "file_path = '../data/wdc/train_large/preprocessed_wdcproducts80cc20rnd000un_train_large.pkl.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate new examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(product_1, product_2, label, explanation, custom_id):\n",
    "    example = {\n",
    "        \"title_left\": product_1,\n",
    "        \"title_right\": product_2,\n",
    "        \"label\": label,\n",
    "        \"explanation\": explanation\n",
    "    }\n",
    "    return {\n",
    "        \"custom_id\": custom_id,\n",
    "        \"method\": \"POST\",\n",
    "        \"url\": \"/v1/chat/completions\",\n",
    "        \"body\": {\n",
    "            \"model\": \"gpt-4o-mini\",\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": f\"\"\"\n",
    "                Please generate 4 similar examples to this one 3 should be non matches 1 should be a match.\n",
    "                \n",
    "                {example}\n",
    "                Only return the title_left, title_right and the label in a JSON format\n",
    "                \"\"\"}\n",
    "            ],\n",
    "            \"max_tokens\": 300,\n",
    "            \"temperature\": 0\n",
    "        }\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your data\n",
    "small_df = pd.read_pickle(file_path, compression=\"gzip\")\n",
    "\n",
    "# Create the JSONL file with all requests\n",
    "requests = []\n",
    "for index, row in tqdm(small_df.iterrows(), total=small_df.shape[0]):\n",
    "    product_1 = row[\"title_left\"]\n",
    "    product_2 = row[\"title_right\"]\n",
    "    label = row[\"label\"]\n",
    "    explanation = row[\"explanation\"]\n",
    "    custom_id = row[\"pair_id\"]\n",
    "    prompt = create_prompt(product_1, product_2, label, explanation, custom_id=custom_id)\n",
    "    requests.append(prompt)\n",
    "\n",
    "batch_file_path = \"batch_input_new_examples_based_train_small.jsonl\"\n",
    "with open(batch_file_path, \"w\") as f:\n",
    "    for request in requests:\n",
    "        f.write(json.dumps(request) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_input_file = client.files.create(\n",
    "    file=open(batch_file_path, \"rb\"),\n",
    "    purpose=\"batch\"\n",
    ")\n",
    "\n",
    "batch_input_file_id = batch_input_file.id\n",
    "\n",
    "batch = client.batches.create(\n",
    "    input_file_id=batch_input_file_id,\n",
    "    endpoint=\"/v1/chat/completions\",\n",
    "    completion_window=\"24h\",\n",
    "    metadata={\"description\": \"Generate new examples based on explanations, small training set\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate examples with demonstartion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_synthetic_examples(product_1, product_2, label, custom_id, examples=None):\n",
    "    label = \"Yes\" if label == 1 else \"NO\"\n",
    "    return {\n",
    "        \"custom_id\": custom_id,\n",
    "        \"method\": \"POST\",\n",
    "        \"url\": \"/v1/chat/completions\",\n",
    "        \"body\": {\n",
    "            \"model\": \"gpt-4o\",\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": f\"\"\"\n",
    "                I'm currently testing large language, models on the task of entity matching. In this context, I am first fine-tuning them, and then testing their weaknesses and strengths. The example I will show you is wrongly classified by the model and that idea is to generate four new examples three of which should be negative, i.e. non-matches, and one of them match. For context, two products are considered to be a match if the two entity descriptions refer to the same real world entity. This does not mean that the descriptions need to be the same but that the entity the decription refers to needs to match. Secondly products are not a match if the two descriptions refer to different products.  As a model has previously made an error on these two entity descriptions it is important to create examples that present a similar challenge. Please focus on corner cases meaning examples that are quite difficult to get correct. The generated examples should belong to the same category as the presented product and should be very similar to it. However even if they are a match the strings should never match exactly. The results should only be presented as JSON containing degenerated entity, one and entity two as well as information if they are a match or not represented by boolean and value. Only return JSON.\n",
    "                \n",
    "                {examples}\n",
    "\n",
    "                Here is the misclassified example:\n",
    "                Entity 1: {product_1}\n",
    "                Entity 2: {product_2}\n",
    "                Label: {label}\n",
    "                \"\"\"}\n",
    "            ],\n",
    "            \"max_tokens\": 2_500,\n",
    "            \"temperature\": 0\n",
    "        }\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimized Cosine Similarity with Matrix Operations\n",
    "def find_most_similar_examples(test_embedding, train_df, top_n=6):\n",
    "    # Convert lists of embeddings to a numpy array if not already\n",
    "    train_embeddings = np.array(list(train_df['embedding'].values))\n",
    "    test_embedding = np.array(test_embedding).reshape(1, -1)\n",
    "    \n",
    "    # Calculate cosine similarities for all train embeddings at once\n",
    "    similarities = cosine_similarity(test_embedding, train_embeddings)\n",
    "    \n",
    "    # Get indices of top_n highest similarities\n",
    "    most_similar_indices = np.argsort(similarities[0])[::-1][:top_n]\n",
    "    most_similar_examples = train_df.iloc[most_similar_indices].to_dict(orient='records')\n",
    "    \n",
    "    return most_similar_examples\n",
    "\n",
    "def transform_label(label):\n",
    "    return \"Yes\" if label == 1 else \"No\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your data\n",
    "validation_df = pd.read_pickle(file_path, compression=\"gzip\")\n",
    "\n",
    "# Create the JSONL file with all requests\n",
    "requests = []\n",
    "for index, row in tqdm(validation_df.iterrows(), total=validation_df.shape[0]):\n",
    "    product_1 = row[\"title_left\"]\n",
    "    product_2 = row[\"title_right\"]\n",
    "    label = row[\"label\"]\n",
    "    custom_id = row[\"pair_id\"]\n",
    "    examples = find_most_similar_examples(row[\"embedding\"], validation_df, top_n=6)\n",
    "    example_1 = \"\"\n",
    "    example_2 = \"\"\n",
    "    \n",
    "    for index, example in enumerate(examples):\n",
    "        if index % 2 == 0:\n",
    "            example_1 = example_1 + \"Entity 1: \" + example[\"title_left\"]\n",
    "            example_1 = example_1 + \"Entity 2: \" + example[\"title_right\"]\n",
    "            example_1 = example_1 + \"Label: \" + transform_label(example[\"label\"])\n",
    "            example_1 = example_1 + \"\\n ---------------- \\n\"\n",
    "            \n",
    "        else:\n",
    "            example_2 = example_2 + \"Entity 1: \" + example[\"title_left\"]\n",
    "            example_2 = example_2 + \"Entity 2: \" + example[\"title_right\"]\n",
    "            example_2 = example_2 + \"Label: \" + transform_label(example[\"label\"])\n",
    "            example_2 = example_2 + \"\\n ---------------- \\n\"\n",
    "        \n",
    "    prompt_1 = create_synthetic_examples(product_1, product_2, label, custom_id=f\"{custom_id}_1\", examples=example_1)\n",
    "    prompt_2 = create_synthetic_examples(product_1, product_2, label, custom_id=f\"{custom_id}_2\",examples=example_2)\n",
    "    requests.append(prompt_1)\n",
    "    requests.append(prompt_2)\n",
    "\n",
    "batch_file_path = \"synthetic_with_explanations.jsonl\"\n",
    "with open(batch_file_path, \"w\") as f:\n",
    "    for request in requests:\n",
    "        f.write(json.dumps(request) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_input_file = client.files.create(\n",
    "  file=open(batch_file_path, \"rb\"),\n",
    "  purpose=\"batch\"\n",
    ")\n",
    "\n",
    "batch_input_file_id = batch_input_file.id\n",
    "\n",
    "batch = client.batches.create(\n",
    "    input_file_id=batch_input_file_id,\n",
    "    endpoint=\"/v1/chat/completions\",\n",
    "    completion_window=\"24h\",\n",
    "    metadata={\"description\": \"Generate synthetic examples\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the synthetic examples\n",
    "batch_json_file = \"\"\n",
    "synthetic_examples = pd.read_json(batch_json_file, lines=True)\n",
    "\n",
    "# Parse the results and update your dataframe\n",
    "parsed_df = synthetic_examples[\"response\"].apply(parse_response)\n",
    "\n",
    "# Concatenate the parsed results with the original dataframe\n",
    "df_results = pd.concat([synthetic_examples, parsed_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to parse the content and extract entities and label\n",
    "def extract_multiple_entities(content):\n",
    "    try:\n",
    "        content = content.replace(\"```json\\n\", \"\").replace(\"\\n```\", \"\")\n",
    "        # Load the JSON string into a Python dictionary (or list if multiple entities are in a list)\n",
    "        data = json.loads(content)\n",
    "        # Assuming the content is a list of dictionaries\n",
    "        rows = []\n",
    "        for entity in data:\n",
    "            title_left = entity.get('entity_one')\n",
    "            title_right = entity.get('entity_two')\n",
    "            label = entity.get('match')\n",
    "            rows.append([title_left, title_right, label])\n",
    "        \n",
    "        return pd.DataFrame(rows, columns=['title_left', 'title_right', 'label'])\n",
    "    \n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        print(\"Error parsing\")\n",
    "        # Handle the case where content is not a valid JSON or is missing\n",
    "        return pd.DataFrame(columns=['title_left', 'title_right', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty DataFrame to store the results\n",
    "expanded_df = pd.DataFrame(columns=['title_left', 'title_right', 'label'])\n",
    "\n",
    "# Iterate over each row in df_results\n",
    "for index, row in df_results.iterrows():\n",
    "    # Parse and extract the multiple entities from the content\n",
    "    expanded_rows = extract_multiple_entities(row['content'])\n",
    "    \n",
    "    # Optionally, add other columns from df_results to the expanded DataFrame\n",
    "    for col in df_results.columns:\n",
    "        if col != 'content':\n",
    "            expanded_rows[col] = row[col]\n",
    "    \n",
    "    # Append the expanded rows to the final DataFrame\n",
    "    expanded_df = pd.concat([expanded_df, expanded_rows], ignore_index=True)\n",
    "    \n",
    "# Save the expanded DataFrame to a new file\n",
    "expanded_df.to_csv(f\"{file_path.replace('.pkl.gz', '')}expanded_synthetic_examples.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
