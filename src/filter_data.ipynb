{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import os\n",
                "from openai import OpenAI\n",
                "\n",
                "from dotenv import load_dotenv\n",
                "import json\n",
                "\n",
                "# Load OPENAI_API_KEY from .env file\n",
                "load_dotenv()\n",
                "\n",
                "client = OpenAI()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Extended filtration approach "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Function to extract the entity strings\n",
                "def extract_entities(text):\n",
                "    entity_1 = text.split(\"Entity 1: '\")[1].split(\"'\")[0]\n",
                "    entity_2 = text.split(\"Entity 2: '\")[1].split(\"'\")[0]\n",
                "    return entity_1, entity_2"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Filtered out 3 rows where entity_1 and entity_2 are the same\n"
                    ]
                }
            ],
            "source": [
                "gpt_filtered = pd.read_csv(\"../data/wdc/filtered/small/wdc_train_small_filtered.csv\")\n",
                "initial_length = len(gpt_filtered)\n",
                "\n",
                "# Apply the function to extract the entities\n",
                "gpt_filtered[['entity_1', 'entity_2']] = gpt_filtered['prompt'].apply(lambda x: pd.Series(extract_entities(x)))\n",
                "\n",
                "# Filter the DataFrame to keep only rows where entity_1 and entity_2 are not the same\n",
                "filtered_df = gpt_filtered[gpt_filtered['entity_1'] != gpt_filtered['entity_2']]\n",
                "\n",
                "# Drop the temporary entity columns if needed\n",
                "#filtered_df = filtered_df.drop(columns=['entity_1', 'entity_2'])\n",
                "\n",
                "print(f\"Filtered out {initial_length - len(filtered_df)} rows where entity_1 and entity_2 are the same\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "entity_1        HDD 35 4TB Seagate IronWolf Pro NAS ST4000NE001\n",
                            "entity_2      HD 3,5 4TB 7200RPM IRONWOLF PRO 128 MB SATA3 S...\n",
                            "completion                                                  Yes\n",
                            "Name: 0, dtype: object"
                        ]
                    },
                    "execution_count": 4,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "filtered_df[[\"entity_1\", \"entity_2\", \"completion\"]].iloc[0]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Filter using ChatGPT"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_prompt(prompt, custom_id):\n",
                "    return {\n",
                "        \"custom_id\": custom_id,\n",
                "        \"method\": \"POST\",\n",
                "        \"url\": \"/v1/chat/completions\",\n",
                "        \"body\": {\n",
                "            \"model\": \"gpt-4o-2024-08-06\",\n",
                "            \"messages\": [\n",
                "                {\"role\": \"user\", \"content\": prompt},\n",
                "            ],\n",
                "            \"max_tokens\": 200,\n",
                "            \"temperature\": 0\n",
                "        }\n",
                "    }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "# reset the index\n",
                "filtered_df = filtered_df.reset_index(drop=True)\n",
                "\n",
                "# loop through the rows and create the prompts\n",
                "prompts = []\n",
                "for index, row in filtered_df.iterrows():\n",
                "    prompt = f\"\"\"\n",
                "        I am creating an entity matching benchmark and need to develop a training split that helps the model learn the intricacies of entity matching. I will provide you with two entity descriptions. Your task is to evaluate whether they form an interesting pair for training purposes. Please limit your response to ‘Yes’ or ‘No’.\n",
                "        \n",
                "        Entity 1: '{row['entity_1']}'\n",
                "        Entity 2: '{row['entity_2']}'\n",
                "    \"\"\"\n",
                "    prompts.append(create_prompt(prompt, str(index)))\n",
                "    \n",
                "# Start a batch request\n",
                "batch_file_path = \"filter.jsonl\"\n",
                "with open(batch_file_path, \"w\") as f:\n",
                "    for request in prompts:\n",
                "        f.write(json.dumps(request) + \"\\n\")\n",
                "        \n",
                "batch_input_file = client.files.create(\n",
                "    file=open(batch_file_path, \"rb\"),\n",
                "    purpose=\"batch\"\n",
                ")\n",
                "\n",
                "batch_input_file_id = batch_input_file.id\n",
                "\n",
                "batch = client.batches.create(\n",
                "    input_file_id=batch_input_file_id,\n",
                "    endpoint=\"/v1/chat/completions\",\n",
                "    completion_window=\"24h\",\n",
                "    metadata={\"description\": \"Filter dataset for entity matching benchmark\"}\n",
                ")\n",
                "\n",
                "# delete the batch input file\n",
                "os.remove(batch_file_path)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Batch(id='batch_wB7usqWGkzTdgAutYd1p1wpH', completion_window='24h', created_at=1724604067, endpoint='/v1/chat/completions', input_file_id='file-WYhSkpzDqdCTcvVzwzRVYfAr', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1724690467, failed_at=None, finalizing_at=None, in_progress_at=None, metadata={'description': 'Filter dataset for entity matching benchmark'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))\n"
                    ]
                }
            ],
            "source": [
                "print(batch)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "def parse_response(response):\n",
                "    body = response.get(\"body\", {})\n",
                "    usage = body.get(\"usage\", {})\n",
                "    choices = body.get(\"choices\", [{}])\n",
                "    message = choices[0].get(\"message\", {}) if choices else {}\n",
                "\n",
                "    return pd.Series({\n",
                "        \"status_code\": response.get(\"status_code\"),\n",
                "        \"request_id\": response.get(\"request_id\"),\n",
                "        \"completion_id\": body.get(\"id\"),\n",
                "        \"created\": body.get(\"created\"),\n",
                "        \"model\": body.get(\"model\"),\n",
                "        \"content\": message.get(\"content\"),\n",
                "        \"prompt_tokens\": usage.get(\"prompt_tokens\"),\n",
                "        \"completion_tokens\": usage.get(\"completion_tokens\"),\n",
                "        \"total_tokens\": usage.get(\"total_tokens\"),\n",
                "    })"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "gpt_answer = pd.read_json(\"../data/wdc/filtered/small/interesting/batch_o7Xo2kSLKc0k1wmwyAyR09Ic_output.jsonl\", lines=True)\n",
                "\n",
                "# Apply the function to the response column\n",
                "parsed_df = gpt_answer[\"response\"].apply(parse_response)\n",
                "\n",
                "# Concatenate the parsed results with the original dataframe\n",
                "df_results = pd.concat([gpt_answer, parsed_df], axis=1)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "content\n",
                            "No     1395\n",
                            "Yes     608\n",
                            "Name: count, dtype: int64"
                        ]
                    },
                    "execution_count": 12,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "df_results[\"content\"].value_counts()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "# filter filtered_df if the index is in the df_results has a content of 'Yes'\n",
                "filtered_df = filtered_df[filtered_df.index.isin(df_results[df_results['content'] == 'Yes'].index)]\n",
                "\n",
                "# reset the index\n",
                "filtered_df = filtered_df.reset_index(drop=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "ename": "NameError",
                    "evalue": "name 'filtered_df' is not defined",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m filtered_df \u001b[38;5;241m=\u001b[39m \u001b[43mfiltered_df\u001b[49m\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentity_1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentity_2\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# save the filtered_df to a csv file\u001b[39;00m\n\u001b[1;32m      3\u001b[0m filtered_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/wdc/filtered/small/interesting/interesting_only.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
                        "\u001b[0;31mNameError\u001b[0m: name 'filtered_df' is not defined"
                    ]
                }
            ],
            "source": [
                "filtered_df = filtered_df.drop(columns=['entity_1', 'entity_2'])\n",
                "# save the filtered_df to a csv file\n",
                "filtered_df.to_csv(\"../data/wdc/filtered/small/interesting/interesting_only.csv\", index=False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [],
            "source": [
                "small_filtered_df = pd.read_csv(\"../data/wdc/filtered/small/wdc_train_small_filtered.csv\")\n",
                "\n",
                "# only keep prompt and completion columns from the filtered_df\n",
                "filtered_df = filtered_df[[\"prompt\", \"completion\"]]\n",
                "\n",
                "# concatenate the small_filtered_df and filtered_df\n",
                "filtered_df = pd.concat([small_filtered_df, filtered_df], axis=0)\n",
                "\n",
                "# save the filtered_df to a csv file\n",
                "filtered_df.to_csv(\"../data/wdc/synthetic/4o/textual_example/interesting/filtered_with_small.csv\", index=False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "completion\n",
                            "No     6718\n",
                            "Yes    2182\n",
                            "Name: count, dtype: int64"
                        ]
                    },
                    "execution_count": 17,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "filtered_df[\"completion\"].value_counts()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>prompt</th>\n",
                            "      <th>completion</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>Do the two entity descriptions refer to the sa...</td>\n",
                            "      <td>Yes</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>Do the two entity descriptions refer to the sa...</td>\n",
                            "      <td>No</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>Do the two entity descriptions refer to the sa...</td>\n",
                            "      <td>No</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>Do the two entity descriptions refer to the sa...</td>\n",
                            "      <td>No</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>Do the two entity descriptions refer to the sa...</td>\n",
                            "      <td>No</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>...</th>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>6889</th>\n",
                            "      <td>Do the two entity descriptions refer to the sa...</td>\n",
                            "      <td>No</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>6890</th>\n",
                            "      <td>Do the two entity descriptions refer to the sa...</td>\n",
                            "      <td>Yes</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>6891</th>\n",
                            "      <td>Do the two entity descriptions refer to the sa...</td>\n",
                            "      <td>No</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>6892</th>\n",
                            "      <td>Do the two entity descriptions refer to the sa...</td>\n",
                            "      <td>Yes</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>6893</th>\n",
                            "      <td>Do the two entity descriptions refer to the sa...</td>\n",
                            "      <td>No</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "<p>10906 rows × 2 columns</p>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "                                                 prompt completion\n",
                            "0     Do the two entity descriptions refer to the sa...        Yes\n",
                            "1     Do the two entity descriptions refer to the sa...         No\n",
                            "2     Do the two entity descriptions refer to the sa...         No\n",
                            "3     Do the two entity descriptions refer to the sa...         No\n",
                            "4     Do the two entity descriptions refer to the sa...         No\n",
                            "...                                                 ...        ...\n",
                            "6889  Do the two entity descriptions refer to the sa...         No\n",
                            "6890  Do the two entity descriptions refer to the sa...        Yes\n",
                            "6891  Do the two entity descriptions refer to the sa...         No\n",
                            "6892  Do the two entity descriptions refer to the sa...        Yes\n",
                            "6893  Do the two entity descriptions refer to the sa...         No\n",
                            "\n",
                            "[10906 rows x 2 columns]"
                        ]
                    },
                    "execution_count": 18,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "filtered_df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "small_filtered = pd.read_csv(\"../data/wdc/filtered/small/wdc_small_filtered.csv\")\n",
                "small = pd.read_pickle(\"../data/wdc/preprocessed_wdcproducts80cc20rnd000un_train_small.pkl.gz\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "# filter out all pair ids from small that are in small_filtered\n",
                "small = small[~small[\"pair_id\"].isin(small_filtered[\"pair_id\"])]\n",
                "# drop embeddings column\n",
                "small = small.drop(columns=[\"embedding\"])\n",
                "small.to_csv(\"../data/wdc/filtered/small/discarded_examples.csv\", index=False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "'Swiss Military Hanowa Flagship 06-5161.2.04.003'"
                        ]
                    },
                    "execution_count": 11,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "small.iloc[0][\"title_right\"]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "env",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.2"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}