{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Metrics Calculation for LLama and GPT Models**\n",
    "\n",
    "This notebook calculates the evaluation metrics (accuracy, F1, precision, and recall) for runs of both LLama and GPT models. After setting the paths to the respective directories, the notebook will generate a `stats.csv` file containing all the metrics for each dataset and task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import helper as analytics\n",
    "from helper import calculate_scores, get_epoch_from_checkpoint\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from utils import parse_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    \n",
    "    return accuracy, f1, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_folders(directory):\n",
    "    checkpoint_folders = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for folder in dirs:\n",
    "            if 'results' in folder:\n",
    "                checkpoint_folders.append(os.path.join(root, folder))\n",
    "                \n",
    "    return checkpoint_folders\n",
    "\n",
    "# Function to extract the checkpoint number\n",
    "def get_checkpoint_number(path):\n",
    "    return int(path.split('-')[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the results for Llama models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing amazon-google-full\n",
      "Processing wdc-fullsize\n",
      "Processing abt-buy-full\n",
      "Processing walmart-amazon\n",
      "Processing dblp-scholar\n",
      "Processing dblp-acm\n"
     ]
    }
   ],
   "source": [
    "RESULT_DIR = \"../results/meta-llama/Meta-Llama-3.1-8B-Instruct/wdc_no_quantization/2024-09-01-17-59-27/results\"\n",
    "\n",
    "experiment_paths = analytics.get_all_files_in_directory(RESULT_DIR)\n",
    "\n",
    "stats_dataframes = []\n",
    "\n",
    "for experiment_path in experiment_paths:\n",
    "    # Load the dataset\n",
    "    dataset_name = experiment_path.split(\"/\")[-2]\n",
    "    print(f\"Processing {dataset_name}\")\n",
    "    df = pd.read_json(experiment_path)\n",
    "    # Calculate stats for the filtered DataFrame\n",
    "    stats_df = analytics.calculate_stats(df)\n",
    "    stats_df['Dataset'] = dataset_name  # Add dataset name for reference\n",
    "    stats_dataframes.append(stats_df)\n",
    "    \n",
    "result_df = pd.concat(stats_dataframes)\n",
    "result_df.to_csv(f\"{RESULT_DIR}/stats.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the results of OpenAPI batch jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main processing function\n",
    "def process_results_gpt(directory):\n",
    "    \"\"\"\n",
    "    Main function to process the results from the .jsonl files in a directory, apply transformations, and compute metrics.\n",
    "    \n",
    "    Parameters:\n",
    "    directory (str): The path to the directory containing the JSONL files.\n",
    "    \"\"\"\n",
    "    # Load all .jsonl files from the directory\n",
    "    gpt_result = analytics.get_all_files_in_directory(directory, \"jsonl\")\n",
    "    # Load and concatenate all JSONL files\n",
    "    all_dataframes = [pd.read_json(path, lines=True) for path in gpt_result]\n",
    "    \n",
    "    gpt_result = pd.concat(all_dataframes, ignore_index=True) if all_dataframes else pd.DataFrame()\n",
    "    \n",
    "    # Ensure the DataFrame is not empty\n",
    "    if not gpt_result.empty:\n",
    "        # Split the custom_id into dataset, task, pair_id, and label\n",
    "        gpt_result[['dataset', 'task', 'pair_id', 'label']] = gpt_result.custom_id.str.split(\";\", expand=True)\n",
    "        gpt_result = gpt_result.drop(columns=['custom_id'])\n",
    "        \n",
    "        # Apply the parse_response function to the response column\n",
    "        parsed_df = gpt_result[\"response\"].apply(parse_response)\n",
    "        \n",
    "        # Concatenate the parsed results with the original DataFrame\n",
    "        gpt_result = pd.concat([gpt_result, parsed_df], axis=1)\n",
    "        \n",
    "        # Transform 'content' to binary (0 or 1 based on \"Yes\")\n",
    "        gpt_result['content'] = gpt_result['content'].apply(lambda x: 1 if \"Yes\" in x else 0)\n",
    "        \n",
    "        # Convert label from string to integer\n",
    "        gpt_result['label'] = gpt_result['label'].astype(int)\n",
    "        \n",
    "        # Group by 'dataset' and 'task', then calculate metrics\n",
    "        results = []\n",
    "        grouped = gpt_result.groupby(['dataset', 'task'])\n",
    "        \n",
    "        for (dataset, task), group in grouped:\n",
    "            y_true = group['label']\n",
    "            y_pred = group['content']\n",
    "            \n",
    "            accuracy, f1, precision, recall = calculate_metrics(y_true, y_pred)\n",
    "            \n",
    "            results.append({\n",
    "                'dataset': dataset,\n",
    "                'task': task,\n",
    "                'accuracy': accuracy,\n",
    "                'f1_score': f1,\n",
    "                'precision': precision,\n",
    "                'recall': recall\n",
    "            })\n",
    "        \n",
    "        # Convert the results into a DataFrame\n",
    "        metrics_df = pd.DataFrame(results)\n",
    "        \n",
    "        # Save metrics to CSV in the same directory\n",
    "        output_path = os.path.join(directory, 'stats.csv')\n",
    "        metrics_df.to_csv(output_path, index=False)\n",
    "        print(f\"Metrics saved to {output_path}\")\n",
    "    else:\n",
    "        print(\"No data found in the provided directory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics saved to ../../results/gpt-4o-mini/dblp-scholar/stats.csv\n"
     ]
    }
   ],
   "source": [
    "# set the directory of the gpt batch job\n",
    "process_results_gpt(\"../results/gpt-4o-mini/dblp-scholar\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
