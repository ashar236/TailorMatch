{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import helper as analytics\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "from helper import calculate_scores, get_epoch_from_checkpoint\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 32860 records from ../../results/gpt-4o-mini/wdc-small-with-synthetic/batch_TK8yBid20mYnB7ziRsYknVg8_output.jsonl\n",
      "Removed 0 records from ../../results/gpt-4o-mini/wdc-small-with-synthetic/batch_7ymSj1XZ6YlwZe4b5amY02v3_output.jsonl\n"
     ]
    }
   ],
   "source": [
    "files = analytics.get_all_files_in_directory(\"../../results/gpt-4o-mini/wdc-small-with-synthetic\", \".jsonl\")\n",
    "\n",
    "for file in files:\n",
    "    df = pd.read_json(file, lines=True)\n",
    "    starting_length = len(df)\n",
    "    # drop record if the custom id includes the words \"dblp-acm\" \"dblp-scholar\"\n",
    "    df = df[~df[\"custom_id\"].str.contains(\"dblp-acm\")]\n",
    "    df = df[~df[\"custom_id\"].str.contains(\"dblp-scholar\")]\n",
    "    ending_length = len(df)\n",
    "    print(f\"Removed {starting_length - ending_length} records from {file}\")\n",
    "    # save the file\n",
    "    df.to_json(f\"{file}\", orient='records', lines=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    \n",
    "    return accuracy, f1, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_checkpoint_folders(directory):\n",
    "    checkpoint_folders = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for folder in dirs:\n",
    "            if 'checkpoint' in folder:\n",
    "                checkpoint_folders.append(os.path.join(root, folder))\n",
    "                \n",
    "    return checkpoint_folders\n",
    "\n",
    "# Function to extract the checkpoint number\n",
    "def get_checkpoint_number(path):\n",
    "    return int(path.split('-')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "included_tasks=[\"domain-complex-free (Product)\", \"domain-simple-free (Product)\", \"domain-complex-force (Product)\", \"domain-simple-force (Product)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULT_DIR = \"../../results/meta-llama/Meta-Llama-3.1-8B-Instruct/abt-buy/no_explanations/2024-08-14-15-50-23/results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing amazon-google-full\n",
      "Processing wdc-fullsize\n",
      "Processing abt-buy-full\n",
      "Processing walmart-amazon\n",
      "Processing dblp-scholar\n",
      "Processing dblp-acm\n"
     ]
    }
   ],
   "source": [
    "experiment_paths = analytics.get_all_files_in_directory(RESULT_DIR)\n",
    "\n",
    "stats_dataframes = []\n",
    "\n",
    "for experiment_path in experiment_paths:\n",
    "    # Load the dataset\n",
    "    dataset_name = experiment_path.split(\"/\")[-2]\n",
    "    print(f\"Processing {dataset_name}\")\n",
    "    df = pd.read_json(experiment_path)\n",
    "    # Calculate stats for the filtered DataFrame\n",
    "    stats_df = analytics.calculate_stats(df)\n",
    "    stats_df['Dataset'] = dataset_name  # Add dataset name for reference\n",
    "    stats_dataframes.append(stats_df)\n",
    "    \n",
    "result_df = pd.concat(stats_dataframes)\n",
    "result_df.to_csv(f\"{RESULT_DIR}/stats.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Task</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Num -1 Responses</th>\n",
       "      <th>Dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>domain-complex-free (Product)</td>\n",
       "      <td>0.898823</td>\n",
       "      <td>0.567164</td>\n",
       "      <td>0.503311</td>\n",
       "      <td>0.649573</td>\n",
       "      <td>0</td>\n",
       "      <td>amazon-google-full</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>domain-simple-free (Product)</td>\n",
       "      <td>0.887484</td>\n",
       "      <td>0.565657</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.717949</td>\n",
       "      <td>0</td>\n",
       "      <td>amazon-google-full</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>domain-complex-force (Product)</td>\n",
       "      <td>0.897078</td>\n",
       "      <td>0.561338</td>\n",
       "      <td>0.496711</td>\n",
       "      <td>0.645299</td>\n",
       "      <td>0</td>\n",
       "      <td>amazon-google-full</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>domain-simple-force (Product)</td>\n",
       "      <td>0.894025</td>\n",
       "      <td>0.591597</td>\n",
       "      <td>0.487535</td>\n",
       "      <td>0.752137</td>\n",
       "      <td>0</td>\n",
       "      <td>amazon-google-full</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>domain-complex-free (Product)</td>\n",
       "      <td>0.930889</td>\n",
       "      <td>0.627545</td>\n",
       "      <td>0.782090</td>\n",
       "      <td>0.524000</td>\n",
       "      <td>0</td>\n",
       "      <td>wdc-fullsize</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>domain-simple-free (Product)</td>\n",
       "      <td>0.931556</td>\n",
       "      <td>0.653933</td>\n",
       "      <td>0.746154</td>\n",
       "      <td>0.582000</td>\n",
       "      <td>0</td>\n",
       "      <td>wdc-fullsize</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>domain-complex-force (Product)</td>\n",
       "      <td>0.928222</td>\n",
       "      <td>0.631699</td>\n",
       "      <td>0.734748</td>\n",
       "      <td>0.554000</td>\n",
       "      <td>0</td>\n",
       "      <td>wdc-fullsize</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>domain-simple-force (Product)</td>\n",
       "      <td>0.932222</td>\n",
       "      <td>0.660734</td>\n",
       "      <td>0.744361</td>\n",
       "      <td>0.594000</td>\n",
       "      <td>0</td>\n",
       "      <td>wdc-fullsize</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>domain-complex-free (Product)</td>\n",
       "      <td>0.971294</td>\n",
       "      <td>0.850949</td>\n",
       "      <td>0.963190</td>\n",
       "      <td>0.762136</td>\n",
       "      <td>0</td>\n",
       "      <td>abt-buy-full</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>domain-simple-free (Product)</td>\n",
       "      <td>0.972338</td>\n",
       "      <td>0.858667</td>\n",
       "      <td>0.952663</td>\n",
       "      <td>0.781553</td>\n",
       "      <td>0</td>\n",
       "      <td>abt-buy-full</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>domain-complex-force (Product)</td>\n",
       "      <td>0.971294</td>\n",
       "      <td>0.854881</td>\n",
       "      <td>0.936416</td>\n",
       "      <td>0.786408</td>\n",
       "      <td>0</td>\n",
       "      <td>abt-buy-full</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>domain-simple-force (Product)</td>\n",
       "      <td>0.974426</td>\n",
       "      <td>0.873385</td>\n",
       "      <td>0.933702</td>\n",
       "      <td>0.820388</td>\n",
       "      <td>0</td>\n",
       "      <td>abt-buy-full</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>domain-complex-free (Product)</td>\n",
       "      <td>0.907272</td>\n",
       "      <td>0.599156</td>\n",
       "      <td>0.505338</td>\n",
       "      <td>0.735751</td>\n",
       "      <td>0</td>\n",
       "      <td>walmart-amazon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>domain-simple-free (Product)</td>\n",
       "      <td>0.901415</td>\n",
       "      <td>0.603922</td>\n",
       "      <td>0.485804</td>\n",
       "      <td>0.797927</td>\n",
       "      <td>0</td>\n",
       "      <td>walmart-amazon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>domain-complex-force (Product)</td>\n",
       "      <td>0.895071</td>\n",
       "      <td>0.580897</td>\n",
       "      <td>0.465625</td>\n",
       "      <td>0.772021</td>\n",
       "      <td>0</td>\n",
       "      <td>walmart-amazon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>domain-simple-force (Product)</td>\n",
       "      <td>0.895559</td>\n",
       "      <td>0.588462</td>\n",
       "      <td>0.467890</td>\n",
       "      <td>0.792746</td>\n",
       "      <td>0</td>\n",
       "      <td>walmart-amazon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>domain-complex-free (Product)</td>\n",
       "      <td>0.860850</td>\n",
       "      <td>0.428878</td>\n",
       "      <td>0.911854</td>\n",
       "      <td>0.280374</td>\n",
       "      <td>0</td>\n",
       "      <td>dblp-scholar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>domain-simple-free (Product)</td>\n",
       "      <td>0.851620</td>\n",
       "      <td>0.355522</td>\n",
       "      <td>0.932540</td>\n",
       "      <td>0.219626</td>\n",
       "      <td>0</td>\n",
       "      <td>dblp-scholar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>domain-complex-force (Product)</td>\n",
       "      <td>0.853535</td>\n",
       "      <td>0.404812</td>\n",
       "      <td>0.833819</td>\n",
       "      <td>0.267290</td>\n",
       "      <td>0</td>\n",
       "      <td>dblp-scholar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>domain-simple-force (Product)</td>\n",
       "      <td>0.851620</td>\n",
       "      <td>0.372607</td>\n",
       "      <td>0.878472</td>\n",
       "      <td>0.236449</td>\n",
       "      <td>0</td>\n",
       "      <td>dblp-scholar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>domain-complex-free (Product)</td>\n",
       "      <td>0.923170</td>\n",
       "      <td>0.742547</td>\n",
       "      <td>0.931973</td>\n",
       "      <td>0.617117</td>\n",
       "      <td>0</td>\n",
       "      <td>dblp-acm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>domain-simple-free (Product)</td>\n",
       "      <td>0.898504</td>\n",
       "      <td>0.617960</td>\n",
       "      <td>0.953052</td>\n",
       "      <td>0.457207</td>\n",
       "      <td>0</td>\n",
       "      <td>dblp-acm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>domain-complex-force (Product)</td>\n",
       "      <td>0.892034</td>\n",
       "      <td>0.583463</td>\n",
       "      <td>0.949239</td>\n",
       "      <td>0.421171</td>\n",
       "      <td>0</td>\n",
       "      <td>dblp-acm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>domain-simple-force (Product)</td>\n",
       "      <td>0.880712</td>\n",
       "      <td>0.514003</td>\n",
       "      <td>0.957055</td>\n",
       "      <td>0.351351</td>\n",
       "      <td>0</td>\n",
       "      <td>dblp-acm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Task  Accuracy  F1 Score  Precision    Recall  \\\n",
       "0   domain-complex-free (Product)  0.898823  0.567164   0.503311  0.649573   \n",
       "1    domain-simple-free (Product)  0.887484  0.565657   0.466667  0.717949   \n",
       "2  domain-complex-force (Product)  0.897078  0.561338   0.496711  0.645299   \n",
       "3   domain-simple-force (Product)  0.894025  0.591597   0.487535  0.752137   \n",
       "0   domain-complex-free (Product)  0.930889  0.627545   0.782090  0.524000   \n",
       "1    domain-simple-free (Product)  0.931556  0.653933   0.746154  0.582000   \n",
       "2  domain-complex-force (Product)  0.928222  0.631699   0.734748  0.554000   \n",
       "3   domain-simple-force (Product)  0.932222  0.660734   0.744361  0.594000   \n",
       "0   domain-complex-free (Product)  0.971294  0.850949   0.963190  0.762136   \n",
       "1    domain-simple-free (Product)  0.972338  0.858667   0.952663  0.781553   \n",
       "2  domain-complex-force (Product)  0.971294  0.854881   0.936416  0.786408   \n",
       "3   domain-simple-force (Product)  0.974426  0.873385   0.933702  0.820388   \n",
       "0   domain-complex-free (Product)  0.907272  0.599156   0.505338  0.735751   \n",
       "1    domain-simple-free (Product)  0.901415  0.603922   0.485804  0.797927   \n",
       "2  domain-complex-force (Product)  0.895071  0.580897   0.465625  0.772021   \n",
       "3   domain-simple-force (Product)  0.895559  0.588462   0.467890  0.792746   \n",
       "0   domain-complex-free (Product)  0.860850  0.428878   0.911854  0.280374   \n",
       "1    domain-simple-free (Product)  0.851620  0.355522   0.932540  0.219626   \n",
       "2  domain-complex-force (Product)  0.853535  0.404812   0.833819  0.267290   \n",
       "3   domain-simple-force (Product)  0.851620  0.372607   0.878472  0.236449   \n",
       "0   domain-complex-free (Product)  0.923170  0.742547   0.931973  0.617117   \n",
       "1    domain-simple-free (Product)  0.898504  0.617960   0.953052  0.457207   \n",
       "2  domain-complex-force (Product)  0.892034  0.583463   0.949239  0.421171   \n",
       "3   domain-simple-force (Product)  0.880712  0.514003   0.957055  0.351351   \n",
       "\n",
       "   Num -1 Responses             Dataset  \n",
       "0                 0  amazon-google-full  \n",
       "1                 0  amazon-google-full  \n",
       "2                 0  amazon-google-full  \n",
       "3                 0  amazon-google-full  \n",
       "0                 0        wdc-fullsize  \n",
       "1                 0        wdc-fullsize  \n",
       "2                 0        wdc-fullsize  \n",
       "3                 0        wdc-fullsize  \n",
       "0                 0        abt-buy-full  \n",
       "1                 0        abt-buy-full  \n",
       "2                 0        abt-buy-full  \n",
       "3                 0        abt-buy-full  \n",
       "0                 0      walmart-amazon  \n",
       "1                 0      walmart-amazon  \n",
       "2                 0      walmart-amazon  \n",
       "3                 0      walmart-amazon  \n",
       "0                 0        dblp-scholar  \n",
       "1                 0        dblp-scholar  \n",
       "2                 0        dblp-scholar  \n",
       "3                 0        dblp-scholar  \n",
       "0                 0            dblp-acm  \n",
       "1                 0            dblp-acm  \n",
       "2                 0            dblp-acm  \n",
       "3                 0            dblp-acm  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset\n",
       "wdc-fullsize    0.720788\n",
       "Name: F1 Score, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.groupby('Dataset')[\"F1 Score\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continues training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_validation(checkpoint_path, new_model_name):\n",
    "    result_file = f\"{checkpoint_path}/validation_results.csv\"\n",
    "    checkpoint_paths = list_checkpoint_folders(checkpoint_path)\n",
    "    # Sorting the list by the checkpoint number\n",
    "    checkpoint_paths = sorted(checkpoint_paths, key=get_checkpoint_number)\n",
    "    \n",
    "    results = []\n",
    "    # get all validation files\n",
    "    for checkpoint_path in checkpoint_paths:\n",
    "        if not os.path.exists(f\"{checkpoint_path}/validation_results.json\"):\n",
    "            print(f\"Validation results already exist for {checkpoint_path}\")\n",
    "            continue\n",
    "        df = pd.read_json(f\"{checkpoint_path}/validation_results.json\")\n",
    "        df.loc[df['chatbot_response_clean'] == -1, 'chatbot_response_clean'] = 0\n",
    "        f1, precision, recall = calculate_scores(df)\n",
    "        epoch = get_epoch_from_checkpoint(checkpoint_paths, int(checkpoint_path.split(\"/\")[-1].replace(\"checkpoint-\", \"\")))\n",
    "        result = {\n",
    "            \"checkpoint_path\": checkpoint_path,\n",
    "            \"checkpoint_number\": checkpoint_path.split(\"/\")[-1].replace(\"checkpoint-\", \"\"),\n",
    "            \"epoch\": epoch,\n",
    "            \"f1\": f1,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall\n",
    "        }\n",
    "        results.append(result)\n",
    "    df = pd.DataFrame(results)\n",
    "\n",
    "    # sort by highest f1\n",
    "    df_sorted = df.sort_values(by='f1', ascending=False)\n",
    "    #df_sorted.to_csv(\"{checkpoint_path}/validation_results.csv\", index=False)\n",
    "\n",
    "    # get the checkpoint path for the best f1\n",
    "    best_checkpoint_path = df_sorted.iloc[0]['checkpoint_path']\n",
    "    print(f\"Best Checkpoint Path: {best_checkpoint_path}\")\n",
    "    print(f\"Best F1: {df_sorted.iloc[0]['f1']}\")\n",
    "    df_sorted.to_csv(result_file, index=False)\n",
    "    return best_checkpoint_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Checkpoint Path: ../../results/meta-llama/Meta-Llama-3.1-8B-Instruct/error/synthetic_textual/2024-08-12-14-59-11_explanation/Meta-Llama-3.1-8B-Instruct-error-small_explanation-enhanced-1/checkpoint-286\n",
      "Best F1: 0.7471852610030706\n",
      "Best Checkpoint Path: ../../results/meta-llama/Meta-Llama-3.1-8B-Instruct/error/synthetic_textual/2024-08-12-14-59-11_explanation/Meta-Llama-3.1-8B-Instruct-error-small_explanation-enhanced-1/checkpoint-286\n",
      "Best Checkpoint Path: ../../results/meta-llama/Meta-Llama-3.1-8B-Instruct/error/synthetic_textual/2024-08-12-14-59-11_explanation/Meta-Llama-3.1-8B-Instruct-error-small_explanation-enhanced-2/checkpoint-143\n",
      "Best F1: 0.7523105360443623\n",
      "Best Checkpoint Path: ../../results/meta-llama/Meta-Llama-3.1-8B-Instruct/error/synthetic_textual/2024-08-12-14-59-11_explanation/Meta-Llama-3.1-8B-Instruct-error-small_explanation-enhanced-2/checkpoint-143\n",
      "Best Checkpoint Path: ../../results/meta-llama/Meta-Llama-3.1-8B-Instruct/error/synthetic_textual/2024-08-12-14-59-11_explanation/Meta-Llama-3.1-8B-Instruct-error-small_explanation-enhanced-3/checkpoint-429\n",
      "Best F1: 0.7022727272727273\n",
      "Best Checkpoint Path: ../../results/meta-llama/Meta-Llama-3.1-8B-Instruct/error/synthetic_textual/2024-08-12-14-59-11_explanation/Meta-Llama-3.1-8B-Instruct-error-small_explanation-enhanced-3/checkpoint-429\n",
      "Best Checkpoint Path: ../../results/meta-llama/Meta-Llama-3.1-8B-Instruct/error/synthetic_textual/2024-08-12-14-59-11_explanation/Meta-Llama-3.1-8B-Instruct-error-small_explanation-enhanced-4/checkpoint-285\n",
      "Best F1: 0.7213822894168467\n",
      "Best Checkpoint Path: ../../results/meta-llama/Meta-Llama-3.1-8B-Instruct/error/synthetic_textual/2024-08-12-14-59-11_explanation/Meta-Llama-3.1-8B-Instruct-error-small_explanation-enhanced-4/checkpoint-285\n",
      "Best Checkpoint Path: ../../results/meta-llama/Meta-Llama-3.1-8B-Instruct/error/synthetic_textual/2024-08-12-14-59-11_explanation/Meta-Llama-3.1-8B-Instruct-error-small_explanation-enhanced-5/checkpoint-285\n",
      "Best F1: 0.766798418972332\n",
      "Best Checkpoint Path: ../../results/meta-llama/Meta-Llama-3.1-8B-Instruct/error/synthetic_textual/2024-08-12-14-59-11_explanation/Meta-Llama-3.1-8B-Instruct-error-small_explanation-enhanced-5/checkpoint-285\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,6):\n",
    "    checkpoint_path = f\"../../results/meta-llama/Meta-Llama-3.1-8B-Instruct/error/synthetic_textual/2024-08-12-14-59-11_explanation/Meta-Llama-3.1-8B-Instruct-error-small_explanation-enhanced-{i}\"\n",
    "    new_model_name = f\"Meta-Llama-3.1-70B-Instruct-error-small_explanation-enhanced-{i}\"\n",
    "    best_checkpoint_path = analyse_validation(checkpoint_path, new_model_name)\n",
    "    print(f\"Best Checkpoint Path: {best_checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_response(response):\n",
    "    body = response.get(\"body\", {})\n",
    "    usage = body.get(\"usage\", {})\n",
    "    choices = body.get(\"choices\", [{}])\n",
    "    message = choices[0].get(\"message\", {}) if choices else {}\n",
    "\n",
    "    return pd.Series({\n",
    "        \"status_code\": response.get(\"status_code\"),\n",
    "        \"request_id\": response.get(\"request_id\"),\n",
    "        \"completion_id\": body.get(\"id\"),\n",
    "        \"created\": body.get(\"created\"),\n",
    "        \"model\": body.get(\"model\"),\n",
    "        \"content\": message.get(\"content\"),\n",
    "        \"prompt_tokens\": usage.get(\"prompt_tokens\"),\n",
    "        \"completion_tokens\": usage.get(\"completion_tokens\"),\n",
    "        \"total_tokens\": usage.get(\"total_tokens\"),\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ft:gpt-4o-mini-2024-07-18:wbsg-uni-mannheim::9z0GiK5t'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json(\"../../results/gpt-4o-mini/tobedetermined/batch_s1Z0ng7isxQqHWGDdTQI70bM_output.jsonl\", lines=True)[\"response\"].apply(parse_response)\n",
    "\n",
    "# calculate the average completion tokens\n",
    "\n",
    "df[\"model\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_result_1_path = \"../../results/gpt-4o-mini/wdc-small-synthetic-filtered-interesting/batch_EHm0OsyYK3mO3l61WDzDwRs6_output.jsonl\"\n",
    "\n",
    "gpt_result_1 = pd.read_json(gpt_result_1_path, lines=True)\n",
    "gpt_result_2 = pd.read_json(\"../../results/gpt-4o-mini/wdc-small-synthetic-filtered-interesting/batch_y1qVVew2QgFfM1qzXuSOo0zD_output.jsonl\", lines=True)\n",
    "\n",
    "# concatenate the two dataframes\n",
    "gpt_result = pd.concat([gpt_result_1, gpt_result_2])\n",
    "# reset the index\n",
    "gpt_result = gpt_result.reset_index(drop=True)\n",
    "\n",
    "# split the custom_id into dataset, task and index\n",
    "gpt_result[['dataset', 'task', 'pair_id', 'label']] = gpt_result.custom_id.str.split(\";\", expand=True)\n",
    "gpt_result = gpt_result.drop(columns=['custom_id'])\n",
    "\n",
    "# Apply the function to the response column\n",
    "parsed_df = gpt_result[\"response\"].apply(parse_response)\n",
    "\n",
    "# Concatenate the parsed results with the original dataframe\n",
    "gpt_result = pd.concat([gpt_result, parsed_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform content to 0 or 1\n",
    "gpt_result['content'] = gpt_result['content'].apply(lambda x: 1 if \"Yes\" in x else 0)\n",
    "\n",
    "# change label from string to int\n",
    "gpt_result['label'] = gpt_result['label'].astype(int)\n",
    "\n",
    "# Assuming 'label' is the true label and 'response' is the predicted label\n",
    "results = []\n",
    "\n",
    "grouped = gpt_result.groupby(['dataset', 'task'])\n",
    "\n",
    "for (dataset, task), group in grouped:\n",
    "    y_true = group['label']\n",
    "    y_pred = group['content']\n",
    "    \n",
    "    accuracy, f1, precision, recall = calculate_metrics(y_true, y_pred)\n",
    "    \n",
    "    results.append({\n",
    "        'dataset': dataset,\n",
    "        'task': task,\n",
    "        'accuracy': accuracy,\n",
    "        'f1_score': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    })\n",
    "\n",
    "# Convert the results into a DataFrame\n",
    "metrics_df = pd.DataFrame(results)\n",
    "metrics_df.to_csv(f\"{gpt_result_1_path.rsplit('/', 1)[0]}/stats.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>task</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abt-buy-full</td>\n",
       "      <td>domain-complex-force (Product)</td>\n",
       "      <td>0.961900</td>\n",
       "      <td>0.843011</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.951456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abt-buy-full</td>\n",
       "      <td>domain-complex-free (Product)</td>\n",
       "      <td>0.959290</td>\n",
       "      <td>0.834746</td>\n",
       "      <td>0.740602</td>\n",
       "      <td>0.956311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abt-buy-full</td>\n",
       "      <td>domain-simple-force (Product)</td>\n",
       "      <td>0.962944</td>\n",
       "      <td>0.844639</td>\n",
       "      <td>0.768924</td>\n",
       "      <td>0.936893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abt-buy-full</td>\n",
       "      <td>domain-simple-free (Product)</td>\n",
       "      <td>0.962944</td>\n",
       "      <td>0.845316</td>\n",
       "      <td>0.766798</td>\n",
       "      <td>0.941748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>amazon-google-full</td>\n",
       "      <td>domain-complex-force (Product)</td>\n",
       "      <td>0.873528</td>\n",
       "      <td>0.594972</td>\n",
       "      <td>0.441909</td>\n",
       "      <td>0.910256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>amazon-google-full</td>\n",
       "      <td>domain-complex-free (Product)</td>\n",
       "      <td>0.873092</td>\n",
       "      <td>0.596394</td>\n",
       "      <td>0.441478</td>\n",
       "      <td>0.918803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>amazon-google-full</td>\n",
       "      <td>domain-simple-force (Product)</td>\n",
       "      <td>0.873092</td>\n",
       "      <td>0.593007</td>\n",
       "      <td>0.440748</td>\n",
       "      <td>0.905983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>amazon-google-full</td>\n",
       "      <td>domain-simple-free (Product)</td>\n",
       "      <td>0.865678</td>\n",
       "      <td>0.580381</td>\n",
       "      <td>0.426000</td>\n",
       "      <td>0.910256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>dblp-acm</td>\n",
       "      <td>domain-complex-force (Product)</td>\n",
       "      <td>0.854832</td>\n",
       "      <td>0.711647</td>\n",
       "      <td>0.553059</td>\n",
       "      <td>0.997748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>dblp-acm</td>\n",
       "      <td>domain-complex-free (Product)</td>\n",
       "      <td>0.852810</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.549751</td>\n",
       "      <td>0.995495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>dblp-acm</td>\n",
       "      <td>domain-simple-force (Product)</td>\n",
       "      <td>0.836636</td>\n",
       "      <td>0.686822</td>\n",
       "      <td>0.523641</td>\n",
       "      <td>0.997748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>dblp-acm</td>\n",
       "      <td>domain-simple-free (Product)</td>\n",
       "      <td>0.824505</td>\n",
       "      <td>0.670713</td>\n",
       "      <td>0.505721</td>\n",
       "      <td>0.995495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>dblp-scholar</td>\n",
       "      <td>domain-complex-force (Product)</td>\n",
       "      <td>0.785963</td>\n",
       "      <td>0.618678</td>\n",
       "      <td>0.463075</td>\n",
       "      <td>0.931776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>dblp-scholar</td>\n",
       "      <td>domain-complex-free (Product)</td>\n",
       "      <td>0.789794</td>\n",
       "      <td>0.623870</td>\n",
       "      <td>0.467976</td>\n",
       "      <td>0.935514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>dblp-scholar</td>\n",
       "      <td>domain-simple-force (Product)</td>\n",
       "      <td>0.766806</td>\n",
       "      <td>0.598741</td>\n",
       "      <td>0.440670</td>\n",
       "      <td>0.933645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>dblp-scholar</td>\n",
       "      <td>domain-simple-free (Product)</td>\n",
       "      <td>0.758795</td>\n",
       "      <td>0.592527</td>\n",
       "      <td>0.432374</td>\n",
       "      <td>0.941121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>walmart-amazon</td>\n",
       "      <td>domain-complex-force (Product)</td>\n",
       "      <td>0.883358</td>\n",
       "      <td>0.591453</td>\n",
       "      <td>0.441327</td>\n",
       "      <td>0.896373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>walmart-amazon</td>\n",
       "      <td>domain-complex-free (Product)</td>\n",
       "      <td>0.881406</td>\n",
       "      <td>0.590219</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.906736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>walmart-amazon</td>\n",
       "      <td>domain-simple-force (Product)</td>\n",
       "      <td>0.883358</td>\n",
       "      <td>0.590051</td>\n",
       "      <td>0.441026</td>\n",
       "      <td>0.891192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>walmart-amazon</td>\n",
       "      <td>domain-simple-free (Product)</td>\n",
       "      <td>0.881894</td>\n",
       "      <td>0.592593</td>\n",
       "      <td>0.438903</td>\n",
       "      <td>0.911917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>wdc-fullsize</td>\n",
       "      <td>domain-complex-force (Product)</td>\n",
       "      <td>0.955556</td>\n",
       "      <td>0.810964</td>\n",
       "      <td>0.768817</td>\n",
       "      <td>0.858000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>wdc-fullsize</td>\n",
       "      <td>domain-complex-free (Product)</td>\n",
       "      <td>0.955778</td>\n",
       "      <td>0.812087</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.860000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>wdc-fullsize</td>\n",
       "      <td>domain-simple-force (Product)</td>\n",
       "      <td>0.956222</td>\n",
       "      <td>0.811483</td>\n",
       "      <td>0.777982</td>\n",
       "      <td>0.848000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>wdc-fullsize</td>\n",
       "      <td>domain-simple-free (Product)</td>\n",
       "      <td>0.956222</td>\n",
       "      <td>0.815023</td>\n",
       "      <td>0.768142</td>\n",
       "      <td>0.868000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               dataset                            task  accuracy  f1_score  \\\n",
       "0         abt-buy-full  domain-complex-force (Product)  0.961900  0.843011   \n",
       "1         abt-buy-full   domain-complex-free (Product)  0.959290  0.834746   \n",
       "2         abt-buy-full   domain-simple-force (Product)  0.962944  0.844639   \n",
       "3         abt-buy-full    domain-simple-free (Product)  0.962944  0.845316   \n",
       "4   amazon-google-full  domain-complex-force (Product)  0.873528  0.594972   \n",
       "5   amazon-google-full   domain-complex-free (Product)  0.873092  0.596394   \n",
       "6   amazon-google-full   domain-simple-force (Product)  0.873092  0.593007   \n",
       "7   amazon-google-full    domain-simple-free (Product)  0.865678  0.580381   \n",
       "8             dblp-acm  domain-complex-force (Product)  0.854832  0.711647   \n",
       "9             dblp-acm   domain-complex-free (Product)  0.852810  0.708333   \n",
       "10            dblp-acm   domain-simple-force (Product)  0.836636  0.686822   \n",
       "11            dblp-acm    domain-simple-free (Product)  0.824505  0.670713   \n",
       "12        dblp-scholar  domain-complex-force (Product)  0.785963  0.618678   \n",
       "13        dblp-scholar   domain-complex-free (Product)  0.789794  0.623870   \n",
       "14        dblp-scholar   domain-simple-force (Product)  0.766806  0.598741   \n",
       "15        dblp-scholar    domain-simple-free (Product)  0.758795  0.592527   \n",
       "16      walmart-amazon  domain-complex-force (Product)  0.883358  0.591453   \n",
       "17      walmart-amazon   domain-complex-free (Product)  0.881406  0.590219   \n",
       "18      walmart-amazon   domain-simple-force (Product)  0.883358  0.590051   \n",
       "19      walmart-amazon    domain-simple-free (Product)  0.881894  0.592593   \n",
       "20        wdc-fullsize  domain-complex-force (Product)  0.955556  0.810964   \n",
       "21        wdc-fullsize   domain-complex-free (Product)  0.955778  0.812087   \n",
       "22        wdc-fullsize   domain-simple-force (Product)  0.956222  0.811483   \n",
       "23        wdc-fullsize    domain-simple-free (Product)  0.956222  0.815023   \n",
       "\n",
       "    precision    recall  \n",
       "0    0.756757  0.951456  \n",
       "1    0.740602  0.956311  \n",
       "2    0.768924  0.936893  \n",
       "3    0.766798  0.941748  \n",
       "4    0.441909  0.910256  \n",
       "5    0.441478  0.918803  \n",
       "6    0.440748  0.905983  \n",
       "7    0.426000  0.910256  \n",
       "8    0.553059  0.997748  \n",
       "9    0.549751  0.995495  \n",
       "10   0.523641  0.997748  \n",
       "11   0.505721  0.995495  \n",
       "12   0.463075  0.931776  \n",
       "13   0.467976  0.935514  \n",
       "14   0.440670  0.933645  \n",
       "15   0.432374  0.941121  \n",
       "16   0.441327  0.896373  \n",
       "17   0.437500  0.906736  \n",
       "18   0.441026  0.891192  \n",
       "19   0.438903  0.911917  \n",
       "20   0.768817  0.858000  \n",
       "21   0.769231  0.860000  \n",
       "22   0.777982  0.848000  \n",
       "23   0.768142  0.868000  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
